{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu117.  CUDA version: 11.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {th.__version__}.  CUDA version: {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import BondEncoder, AtomEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "device = th.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import repeat_experiments, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.dgl.ai/tutorials/blitz/6_load_data.html#creating-a-dataset-for-graph-classification-from-csv\n",
    "class MolHIVDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='ogbg-molhiv')\n",
    "        \n",
    "    def _load(self):\n",
    "        self.dataset = DglGraphPropPredDataset(name=\"ogbg-molhiv\", root = 'dataset/')\n",
    "        self.split_idx = self.dataset.get_idx_split()\n",
    "        print(self.dataset.meta_info)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset.graphs[i], self.dataset.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tasks                                                                1\n",
      "eval metric                                                         rocauc\n",
      "download_name                                                          hiv\n",
      "version                                                                  1\n",
      "url                      http://snap.stanford.edu/ogb/data/graphproppre...\n",
      "add_inverse_edge                                                      True\n",
      "data type                                                              mol\n",
      "has_node_attr                                                         True\n",
      "has_edge_attr                                                         True\n",
      "task type                                            binary classification\n",
      "num classes                                                              2\n",
      "split                                                             scaffold\n",
      "additional node files                                                 None\n",
      "additional edge files                                                 None\n",
      "binary                                                               False\n",
      "Name: ogbg-molhiv, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41127"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MolHIVDataset()\n",
    "len(dataset) #41127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/val/test split dataloaders\n",
    "split_idx = dataset.split_idx\n",
    "\n",
    "train_sampler = SubsetRandomSampler(split_idx['train'])\n",
    "val_subset = Subset(dataset, split_idx['valid'])\n",
    "test_subset = Subset(dataset, split_idx['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=bs, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    val_subset, shuffle=False, batch_size=bs, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    test_subset, shuffle=False, batch_size=bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4113, 4113)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_subset), len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=706, num_edges=1512,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=22, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=25, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=20, num_edges=40,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=11, num_edges=20,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=36, num_edges=78,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=14, num_edges=32,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=18, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=27, num_edges=60,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=20,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=22, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=76,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=38, num_edges=84,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=44,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=17, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=44,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=17, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=26,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=32, num_edges=72,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=24, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=20, num_edges=46,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=26, num_edges=58,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=29, num_edges=64,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=72,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=40,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=14, num_edges=30,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=24, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=15, num_edges=30,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=42,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=24,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.unbatch(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 0, 2,  ..., 2, 0, 0],\n",
       "        [5, 0, 4,  ..., 2, 0, 0],\n",
       "        [5, 0, 3,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [6, 0, 3,  ..., 1, 1, 1],\n",
       "        [5, 0, 3,  ..., 1, 1, 1],\n",
       "        [7, 0, 1,  ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndata - node data\n",
    "batch[0].ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 1],\n",
       "        [3, 0, 1],\n",
       "        [3, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edata - edge data\n",
    "batch[0].edata['feat']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [AtomEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L7) and [BondEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parts:\n",
    "## GIN layer\n",
    "## Node convolution\n",
    "## Graph convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Isomorphic Network layer\n",
    "\n",
    "[How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826)\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\mathbf{h}_j^{(l)} ]$$\n",
    "\n",
    "see https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py#L31\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\text{ReLU}(\\mathbf{h}_j^{(l)} + \\mathbf{e}_{ij}) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_{u} - source node\n",
    "# copy_{v} - destination, neighbour node\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GINLayer, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim), \n",
    "            nn.BatchNorm1d(2*emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(2*emb_dim, emb_dim) #from above\n",
    "        )\n",
    "        self.eps = nn.Parameter(th.FloatTensor([0]))\n",
    "        \n",
    "\n",
    "    def reset_parameters(self): # reset the parameters of the MLP\n",
    "        # MLP\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "        # EPS\n",
    "        nn.init.constant_(self.eps, 0.)\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        # Add \"eps\" to the string representation\n",
    "        return f'(eps): nn.Parameter'\n",
    "        \n",
    "    def forward(self, g, x_node, x_edge):\n",
    "        \"\"\"\n",
    "        g : The graph used for message passing\n",
    "        x_node : AtomEncodings\n",
    "        x_edge : BondEncodings\n",
    "        \"\"\"\n",
    "        \n",
    "        with g.local_scope():\n",
    "            # Store edge features to 'bond' key in g.edata\n",
    "            g.edata['bond'] = x_edge\n",
    "            # Store node features to 'x' key in g.ndata\n",
    "            g.srcdata['x'] = x_node\n",
    "            \n",
    "            # calculate the right part h_i = sum_j( F.relu (h_j + e_ij) )\n",
    "                \n",
    "            ## 1. take the sum\n",
    "            g.apply_edges( # see https://docs.dgl.ai/api/python/dgl.function.html#dgl-built-in-function\n",
    "                fn.u_add_e('x', 'bond', 'm')\n",
    "            )\n",
    "            ##2. apply ReLU\n",
    "            g.edata['m'] = F.relu(g.edata['m'])\n",
    "            \n",
    "            ## 3. sum the modified messages https://docs.dgl.ai/en/1.0.x/generated/dgl.DGLGraph.update_all.html\n",
    "            g.update_all(\n",
    "                fn.copy_e('m', 'm'), # creating messages\n",
    "                fn.sum('m', 'mp'), # sending messages to message passing\n",
    "            )\n",
    "            ## extract the final output into variable \"h_mp\"\n",
    "            h_mp = g.dstdata['mp'] # will be used in the next layer\n",
    "                    \n",
    "        # GIN update equation\n",
    "        out = self.mlp((1 + self.eps) * x_node + h_mp)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINLayer(\n",
       "  (eps): nn.Parameter\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "    (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin = GINLayer(emb_dim)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#362101\n",
    "sum(p.numel() for p in gin.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/repos/dgl2/venv/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9681, -0.6240, -0.1038,  ..., -0.6000, -0.9485, -0.7573],\n",
       "        [ 0.3010, -0.6838,  0.5443,  ..., -0.0686, -0.7225, -0.6984],\n",
       "        [ 0.0730,  0.0048, -0.0595,  ..., -0.2102, -0.4386, -0.5446],\n",
       "        ...,\n",
       "        [ 0.4252,  0.1886, -0.2115,  ..., -0.2981, -0.3695, -0.0432],\n",
       "        [ 0.1600, -0.1270, -0.0809,  ..., -0.1951, -0.2563, -0.3816],\n",
       "        [-0.1221, -0.7057,  0.3895,  ..., -0.7057, -0.6778, -0.4727]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/dmlc/dgl/blob/master/examples/mxnet/gin/gin.py\n",
    "\n",
    "gin(\n",
    "    batch[0], \n",
    "    AtomEncoder(emb_dim)(batch[0].ndata['feat']), \n",
    "    BondEncoder(emb_dim)(batch[0].edata['feat'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Node convolution\n",
    "\n",
    "class NodeGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, dropout):\n",
    "        super(NodeGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINLayer(emb_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        for emb in self.atom_encoder.atom_embedding_list: # Atom embeddings\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        \n",
    "        for emb in self.bond_encoder.bond_embedding_list: # Bond embeddings\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        \n",
    "        # reset parameters for the GIN and batch norm layers\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.batch_norms:\n",
    "            bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, g):\n",
    "        \"\"\"        \n",
    "        (node_embeddings, edge_embeddings) -> GIN -> Batch Norm -> ReLU -> Dropout\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert integer categorical features to embeddings\n",
    "        h = self.atom_encoder(g.ndata['feat']) # initial node embeddings\n",
    "        edge_embedding = self.bond_encoder(g.edata['feat']) # edge embeddings\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            h = self.convs[layer](g, h, edge_embedding)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layers - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.dropout, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeGNN(\n",
       "  (atom_encoder): AtomEncoder(\n",
       "    (atom_embedding_list): ModuleList(\n",
       "      (0): Embedding(119, 300)\n",
       "      (1): Embedding(5, 300)\n",
       "      (2-3): 2 x Embedding(12, 300)\n",
       "      (4): Embedding(10, 300)\n",
       "      (5-6): 2 x Embedding(6, 300)\n",
       "      (7-8): 2 x Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (bond_encoder): BondEncoder(\n",
       "    (bond_embedding_list): ModuleList(\n",
       "      (0): Embedding(5, 300)\n",
       "      (1): Embedding(6, 300)\n",
       "      (2): Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "node_gnn = NodeGNN(emb_dim, 1, 0.5)\n",
    "node_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418801"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#418501\n",
    "sum(p.numel() for p in node_gnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.3458, -2.4443,  ...,  0.0000, -1.9417,  0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -1.2409],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.5370, -0.0000, -0.5971],\n",
       "        ...,\n",
       "        [-0.0000, -0.0000, -2.1739,  ..., -0.3948,  3.4321,  0.4168],\n",
       "        [-0.0000, -0.0000, -0.0000,  ..., -1.7384, -2.3997,  1.1329],\n",
       "        [-0.0000, -0.5160,  0.0962,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_gnn(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph convolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{h}_\\mathcal{G} = \\frac{1}{|V_\\mathcal{G}|}\\sum_{j \\in V_\\mathcal{G}} \\mathbf{h}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, node_cls, dropout):\n",
    "        super(GraphGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.node_GNN = node_cls(emb_dim, num_layers, dropout) # node-level GNN, returns node embeddings for every single graph\n",
    "        #node representation -> graph representation\n",
    "        self.graph_pred_linear = nn.Linear(emb_dim, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.node_GNN.reset_parameters()\n",
    "        self.graph_pred_linear.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, g): # !!! g is a batched super-graph\n",
    "        h_node = self.node_GNN(g) # node-level embeddings with self.node_GNN\n",
    "        \n",
    "        # pool the node-level embedding to get graph representations\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h_node # store node data in h\n",
    "            h_graph = dgl.mean_nodes(g, 'h') # average the nodes in the component graphs 32 graph layer embs\n",
    "        \n",
    "        # generate a final prediction by sending the graph-level representation\n",
    "        # through self.graph_pred_linear\n",
    "        pred = self.graph_pred_linear(h_graph)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "dropout = 0.5\n",
    "\n",
    "model = GraphGNN(emb_dim, num_layers, NodeGNN, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphGNN(\n",
       "  (node_GNN): NodeGNN(\n",
       "    (atom_encoder): AtomEncoder(\n",
       "      (atom_embedding_list): ModuleList(\n",
       "        (0): Embedding(119, 300)\n",
       "        (1): Embedding(5, 300)\n",
       "        (2-3): 2 x Embedding(12, 300)\n",
       "        (4): Embedding(10, 300)\n",
       "        (5-6): 2 x Embedding(6, 300)\n",
       "        (7-8): 2 x Embedding(2, 300)\n",
       "      )\n",
       "    )\n",
       "    (bond_encoder): BondEncoder(\n",
       "      (bond_embedding_list): ModuleList(\n",
       "        (0): Embedding(5, 300)\n",
       "        (1): Embedding(6, 300)\n",
       "        (2): Embedding(2, 300)\n",
       "      )\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0-4): 5 x GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1869606\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_runs = 10\n",
    "train_args = dict(epochs=100, lr=0.001, eval_steps=1, log_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 0.1936, Train: 0.7169 AUC, Valid: 0.6502 AUC, Test: 0.6184 AUC\n",
      "---\n",
      "Run: 01, Epoch: 02, Loss: 0.1511, Train: 0.7228 AUC, Valid: 0.6945 AUC, Test: 0.6963 AUC\n",
      "---\n",
      "Run: 01, Epoch: 03, Loss: 0.1477, Train: 0.7572 AUC, Valid: 0.7510 AUC, Test: 0.7142 AUC\n",
      "---\n",
      "Run: 01, Epoch: 04, Loss: 0.1458, Train: 0.7318 AUC, Valid: 0.7256 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 01, Epoch: 05, Loss: 0.1433, Train: 0.7547 AUC, Valid: 0.7528 AUC, Test: 0.7468 AUC\n",
      "---\n",
      "Run: 01, Epoch: 06, Loss: 0.1420, Train: 0.7616 AUC, Valid: 0.7648 AUC, Test: 0.6973 AUC\n",
      "---\n",
      "Run: 01, Epoch: 07, Loss: 0.1413, Train: 0.7860 AUC, Valid: 0.7818 AUC, Test: 0.7124 AUC\n",
      "---\n",
      "Run: 01, Epoch: 08, Loss: 0.1389, Train: 0.7822 AUC, Valid: 0.7881 AUC, Test: 0.7458 AUC\n",
      "---\n",
      "Run: 01, Epoch: 09, Loss: 0.1370, Train: 0.7683 AUC, Valid: 0.7512 AUC, Test: 0.7417 AUC\n",
      "---\n",
      "Run: 01, Epoch: 10, Loss: 0.1361, Train: 0.7930 AUC, Valid: 0.7913 AUC, Test: 0.7369 AUC\n",
      "---\n",
      "Run: 01, Epoch: 11, Loss: 0.1361, Train: 0.7796 AUC, Valid: 0.7827 AUC, Test: 0.7130 AUC\n",
      "---\n",
      "Run: 01, Epoch: 12, Loss: 0.1349, Train: 0.8006 AUC, Valid: 0.7864 AUC, Test: 0.7323 AUC\n",
      "---\n",
      "Run: 01, Epoch: 13, Loss: 0.1346, Train: 0.8092 AUC, Valid: 0.7831 AUC, Test: 0.7650 AUC\n",
      "---\n",
      "Run: 01, Epoch: 14, Loss: 0.1321, Train: 0.8068 AUC, Valid: 0.8034 AUC, Test: 0.7501 AUC\n",
      "---\n",
      "Run: 01, Epoch: 15, Loss: 0.1302, Train: 0.8035 AUC, Valid: 0.7902 AUC, Test: 0.7327 AUC\n",
      "---\n",
      "Run: 01, Epoch: 16, Loss: 0.1293, Train: 0.8191 AUC, Valid: 0.7987 AUC, Test: 0.7713 AUC\n",
      "---\n",
      "Run: 01, Epoch: 17, Loss: 0.1278, Train: 0.8153 AUC, Valid: 0.7935 AUC, Test: 0.7492 AUC\n",
      "---\n",
      "Run: 01, Epoch: 18, Loss: 0.1274, Train: 0.8172 AUC, Valid: 0.7754 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 01, Epoch: 19, Loss: 0.1255, Train: 0.8285 AUC, Valid: 0.8011 AUC, Test: 0.7397 AUC\n",
      "---\n",
      "Run: 01, Epoch: 20, Loss: 0.1259, Train: 0.8265 AUC, Valid: 0.7921 AUC, Test: 0.7469 AUC\n",
      "---\n",
      "Run: 01, Epoch: 21, Loss: 0.1242, Train: 0.8306 AUC, Valid: 0.7923 AUC, Test: 0.7369 AUC\n",
      "---\n",
      "Run: 01, Epoch: 22, Loss: 0.1245, Train: 0.8428 AUC, Valid: 0.8132 AUC, Test: 0.7356 AUC\n",
      "---\n",
      "Run: 01, Epoch: 23, Loss: 0.1219, Train: 0.8479 AUC, Valid: 0.8091 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 01, Epoch: 24, Loss: 0.1234, Train: 0.8493 AUC, Valid: 0.8176 AUC, Test: 0.7407 AUC\n",
      "---\n",
      "Run: 01, Epoch: 25, Loss: 0.1221, Train: 0.8501 AUC, Valid: 0.8036 AUC, Test: 0.7287 AUC\n",
      "---\n",
      "Run: 01, Epoch: 26, Loss: 0.1206, Train: 0.8549 AUC, Valid: 0.8093 AUC, Test: 0.7543 AUC\n",
      "---\n",
      "Run: 01, Epoch: 27, Loss: 0.1205, Train: 0.8507 AUC, Valid: 0.8280 AUC, Test: 0.7313 AUC\n",
      "---\n",
      "Run: 01, Epoch: 28, Loss: 0.1196, Train: 0.8416 AUC, Valid: 0.8047 AUC, Test: 0.7536 AUC\n",
      "---\n",
      "Run: 01, Epoch: 29, Loss: 0.1182, Train: 0.8554 AUC, Valid: 0.7959 AUC, Test: 0.7521 AUC\n",
      "---\n",
      "Run: 01, Epoch: 30, Loss: 0.1178, Train: 0.8562 AUC, Valid: 0.7726 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 01, Epoch: 31, Loss: 0.1163, Train: 0.8686 AUC, Valid: 0.8138 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 01, Epoch: 32, Loss: 0.1162, Train: 0.8655 AUC, Valid: 0.8022 AUC, Test: 0.7486 AUC\n",
      "---\n",
      "Run: 01, Epoch: 33, Loss: 0.1164, Train: 0.8714 AUC, Valid: 0.8186 AUC, Test: 0.7459 AUC\n",
      "---\n",
      "Run: 01, Epoch: 34, Loss: 0.1144, Train: 0.8698 AUC, Valid: 0.8019 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 01, Epoch: 35, Loss: 0.1155, Train: 0.8761 AUC, Valid: 0.8129 AUC, Test: 0.7489 AUC\n",
      "---\n",
      "Run: 01, Epoch: 36, Loss: 0.1142, Train: 0.8807 AUC, Valid: 0.7875 AUC, Test: 0.7290 AUC\n",
      "---\n",
      "Run: 01, Epoch: 37, Loss: 0.1138, Train: 0.8754 AUC, Valid: 0.7933 AUC, Test: 0.7433 AUC\n",
      "---\n",
      "Run: 01, Epoch: 38, Loss: 0.1132, Train: 0.8871 AUC, Valid: 0.8029 AUC, Test: 0.7462 AUC\n",
      "---\n",
      "Run: 01, Epoch: 39, Loss: 0.1121, Train: 0.8835 AUC, Valid: 0.7928 AUC, Test: 0.7427 AUC\n",
      "---\n",
      "Run: 01, Epoch: 40, Loss: 0.1129, Train: 0.8896 AUC, Valid: 0.7977 AUC, Test: 0.7508 AUC\n",
      "---\n",
      "Run: 01, Epoch: 41, Loss: 0.1100, Train: 0.8782 AUC, Valid: 0.8174 AUC, Test: 0.7455 AUC\n",
      "---\n",
      "Run: 01, Epoch: 42, Loss: 0.1116, Train: 0.8850 AUC, Valid: 0.8121 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 01, Epoch: 43, Loss: 0.1095, Train: 0.8889 AUC, Valid: 0.8152 AUC, Test: 0.7479 AUC\n",
      "---\n",
      "Run: 01, Epoch: 44, Loss: 0.1095, Train: 0.8891 AUC, Valid: 0.7882 AUC, Test: 0.7515 AUC\n",
      "---\n",
      "Run: 01, Epoch: 45, Loss: 0.1091, Train: 0.8974 AUC, Valid: 0.8114 AUC, Test: 0.7668 AUC\n",
      "---\n",
      "Run: 01, Epoch: 46, Loss: 0.1085, Train: 0.8963 AUC, Valid: 0.7816 AUC, Test: 0.7497 AUC\n",
      "---\n",
      "Run: 01, Epoch: 47, Loss: 0.1086, Train: 0.8993 AUC, Valid: 0.7963 AUC, Test: 0.7430 AUC\n",
      "---\n",
      "Run: 01, Epoch: 48, Loss: 0.1077, Train: 0.8908 AUC, Valid: 0.7806 AUC, Test: 0.7504 AUC\n",
      "---\n",
      "Run: 01, Epoch: 49, Loss: 0.1083, Train: 0.8998 AUC, Valid: 0.8083 AUC, Test: 0.7628 AUC\n",
      "---\n",
      "Run: 01, Epoch: 50, Loss: 0.1066, Train: 0.9030 AUC, Valid: 0.7990 AUC, Test: 0.7650 AUC\n",
      "---\n",
      "Run: 01, Epoch: 51, Loss: 0.1062, Train: 0.8999 AUC, Valid: 0.8061 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 01, Epoch: 52, Loss: 0.1047, Train: 0.9042 AUC, Valid: 0.7844 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 01, Epoch: 53, Loss: 0.1054, Train: 0.9018 AUC, Valid: 0.7826 AUC, Test: 0.7638 AUC\n",
      "---\n",
      "Run: 01, Epoch: 54, Loss: 0.1053, Train: 0.9129 AUC, Valid: 0.7840 AUC, Test: 0.7708 AUC\n",
      "---\n",
      "Run: 01, Epoch: 55, Loss: 0.1033, Train: 0.9117 AUC, Valid: 0.7851 AUC, Test: 0.7547 AUC\n",
      "---\n",
      "Run: 01, Epoch: 56, Loss: 0.1031, Train: 0.9131 AUC, Valid: 0.7917 AUC, Test: 0.7529 AUC\n",
      "---\n",
      "Run: 01, Epoch: 57, Loss: 0.1035, Train: 0.9166 AUC, Valid: 0.7914 AUC, Test: 0.7526 AUC\n",
      "---\n",
      "Run: 01, Epoch: 58, Loss: 0.1015, Train: 0.9212 AUC, Valid: 0.7832 AUC, Test: 0.7605 AUC\n",
      "---\n",
      "Run: 01, Epoch: 59, Loss: 0.1015, Train: 0.9126 AUC, Valid: 0.7835 AUC, Test: 0.7581 AUC\n",
      "---\n",
      "Run: 01, Epoch: 60, Loss: 0.1018, Train: 0.9243 AUC, Valid: 0.7675 AUC, Test: 0.7647 AUC\n",
      "---\n",
      "Run: 01, Epoch: 61, Loss: 0.1009, Train: 0.9200 AUC, Valid: 0.7907 AUC, Test: 0.7425 AUC\n",
      "---\n",
      "Run: 01, Epoch: 62, Loss: 0.1000, Train: 0.9236 AUC, Valid: 0.7768 AUC, Test: 0.7442 AUC\n",
      "---\n",
      "Run: 01, Epoch: 63, Loss: 0.1012, Train: 0.9226 AUC, Valid: 0.7969 AUC, Test: 0.7557 AUC\n",
      "---\n",
      "Run: 01, Epoch: 64, Loss: 0.0994, Train: 0.9292 AUC, Valid: 0.7859 AUC, Test: 0.7588 AUC\n",
      "---\n",
      "Run: 01, Epoch: 65, Loss: 0.0994, Train: 0.9213 AUC, Valid: 0.7921 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 01, Epoch: 66, Loss: 0.0992, Train: 0.9266 AUC, Valid: 0.7834 AUC, Test: 0.7524 AUC\n",
      "---\n",
      "Run: 01, Epoch: 67, Loss: 0.0971, Train: 0.9351 AUC, Valid: 0.7979 AUC, Test: 0.7506 AUC\n",
      "---\n",
      "Run: 01, Epoch: 68, Loss: 0.0986, Train: 0.9347 AUC, Valid: 0.7902 AUC, Test: 0.7638 AUC\n",
      "---\n",
      "Run: 01, Epoch: 69, Loss: 0.0963, Train: 0.9253 AUC, Valid: 0.7698 AUC, Test: 0.7550 AUC\n",
      "---\n",
      "Run: 01, Epoch: 70, Loss: 0.0959, Train: 0.9351 AUC, Valid: 0.7884 AUC, Test: 0.7414 AUC\n",
      "---\n",
      "Run: 01, Epoch: 71, Loss: 0.0959, Train: 0.9282 AUC, Valid: 0.7752 AUC, Test: 0.7444 AUC\n",
      "---\n",
      "Run: 01, Epoch: 72, Loss: 0.0953, Train: 0.9279 AUC, Valid: 0.7803 AUC, Test: 0.7618 AUC\n",
      "---\n",
      "Run: 01, Epoch: 73, Loss: 0.0952, Train: 0.9370 AUC, Valid: 0.7822 AUC, Test: 0.7545 AUC\n",
      "---\n",
      "Run: 01, Epoch: 74, Loss: 0.0941, Train: 0.9368 AUC, Valid: 0.7721 AUC, Test: 0.7454 AUC\n",
      "---\n",
      "Run: 01, Epoch: 75, Loss: 0.0928, Train: 0.9432 AUC, Valid: 0.7786 AUC, Test: 0.7572 AUC\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "logger = repeat_experiments(\n",
    "    model, train_dataloader, val_dataloader, test_dataloader, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance\n",
    "logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_auc_lb = 0.8232\n",
    "val_auc_lb_std = 0.0090\n",
    "test_auc_lb = 0.7558\n",
    "test_auc_lb_std = 0.0140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further reading\n",
    "# https://arxiv.org/abs/1704.01212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
